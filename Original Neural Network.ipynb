{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "w1 = w11 = np.random.rand()\n",
    "w3 = -w11\n",
    "w2 = w12 = np.random.rand()\n",
    "w4  = -w12\n",
    "w5 = w21 = np.random.rand()\n",
    "w7 = -w21\n",
    "w6 = w22 = np.random.rand()\n",
    "w8 = -w22\n",
    "\n",
    "a=np.array([[w1,w2],[w3,w4],[w5,w6],[w7,w8]])\n",
    "# print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funtion generates data \n",
    "As = []\n",
    "for condition_number in [1,0.1,0.01,0.0001]: # create a list of As with different conditional number\n",
    "        A = np.matrix([[1, 1], [1, 1+condition_number]]) \n",
    "        As.append(A)\n",
    "        \n",
    "def data_generator(n):\n",
    "    #np.random.seed(42)\n",
    "    \n",
    "#     As = []\n",
    "#     for condition_number in [1,0.1,0.01,0.0001]: # create a list of As with different conditional number\n",
    "#         A = np.matrix([[1, 1], [1, 1+condition_number]]) \n",
    "#         As.append(A)\n",
    " \n",
    "    x = np.random.normal(0,1,size = (2,n)) # n is the size of sample\n",
    "    noise = np.random.normal(0,0.04**2) \n",
    "        \n",
    "    y1 = np.dot(As[0],x) + 0.01*noise #compute y with different A\n",
    "    y01 = np.dot(As[1],x) + 0.01*noise\n",
    "    y001 = np.dot(As[2],x) + 0.01*noise\n",
    "    y0001 = np.dot(As[3],x) + 0.01*noise\n",
    "    \n",
    "    x = torch.Tensor(x).float() #convert all ndarrays to tensors\n",
    "    y1 = torch.Tensor(y1).float()\n",
    "    y01 = torch.Tensor(y01).float()\n",
    "    y001 = torch.Tensor(y001).float()\n",
    "    y0001 = torch.Tensor(y0001).float()\n",
    "#     print(x.size())\n",
    "#     print(y1.size())\n",
    "    \n",
    "    return x, y1, y01, y001, y0001    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tikhonov(n):\n",
    "    train_x_all, train_y_1,train_y_01,train_y_001,train_y_0001= data_generator(n)\n",
    "    print(\"train_x_all-----\",train_x_all)\n",
    "    print(\"train_y_1-----\",[train_y_1,train_y_01,train_y_001,train_y_0001])\n",
    "\n",
    "    for index,Y in enumerate([train_y_1,train_y_01,train_y_001,train_y_0001]):\n",
    "        globals()['x_tik_'+str(index)] = np.matmul(np.matmul(inv(np.matmul(As[index].T,As[index])+0.04**2*np.eye(2)),As[index].T),Y)\n",
    "    \n",
    "    for index,X_tik in enumerate([x_tik_0,x_tik_1,x_tik_2,x_tik_3]):\n",
    "        globals()['Error_x_tik_'+str(index)] = (torch.norm(train_x_all-X_tik))**2 * (1./n)\n",
    "    \n",
    "    print(\"Error_tik_1-----\",Error_x_tik_0)\n",
    "    print(\"Error_tik_01-----\",Error_x_tik_1)\n",
    "    print(\"Error_tik_001-----\",Error_x_tik_2)\n",
    "    print(\"Error_tik_0001-----\",Error_x_tik_3)\n",
    "    return x_tik_0,x_tik_1,x_tik_2,x_tik_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tikhonov(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def dRelu(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net:\n",
    "    def __init__(self, x, y):\n",
    "        self.X=x\n",
    "        self.Y=y\n",
    "        self.Yh=np.zeros((1,self.Y.shape[1]))\n",
    "        self.weights = {}\n",
    "        self.ch = {} # a cache variable\n",
    "        self.loss = []\n",
    "        self.lr=0.03\n",
    "        self.size = self.Y.shape[1]\n",
    "    def nInit(self):    \n",
    "        np.random.seed(1)\n",
    "        self.weights['W1'] = torch.from_numpy(np.array([[w1,w2],[w3,w4],[w5,w6],[w7,w8]]))  #(4,2)  \n",
    "        self.weights['W2'] = torch.from_numpy(np.array([[1,-1,0,0],[0,0,1,-1]]))  #(2,4)   \n",
    "        return\n",
    "\n",
    "    def forward(self):    \n",
    "        Z1 = np.matmul(self.weights['W1'],self.X) #self.weights['W1'].dot(self.X)\n",
    "        A1 = Relu(Z1)\n",
    "        self.ch['Z1'],self.ch['A1']=Z1,A1\n",
    "        \n",
    "        Z2 = np.matmul(self.weights['W2'],A1) #self.weights['W2'].dot(A1)\n",
    "        self.ch['Z2'] = Z2\n",
    "        self.Yh=Z2\n",
    "        loss=self.calculate_loss(Z2)\n",
    "        return self.Yh, loss\n",
    "    \n",
    "    def calculate_loss(self,Yh):\n",
    "        loss = (torch.norm(self.Y-self.Yh))**2 * (1./self.size) #norm\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        #Divide the Yh matrix and the X matrix into 2 rows, and assign them to different variables\n",
    "        Yh1 = self.Yh[0] #(1xn)matrix size\n",
    "        Yh2 = self.Yh[1] #(1xn)\n",
    "        X1 = self.X[0] #(1xn)\n",
    "        X2 = self.X[1] #(1xn)\n",
    "        \n",
    "        dLoss_Yh = -2*(self.Y - self.Yh)* (1./self.size) #(2,n)\n",
    "        dloss_yh1 =  dLoss_Yh[0]\n",
    "        dloss_yh2 = dLoss_Yh[1]\n",
    "\n",
    "        \n",
    "        dyh1_A1 = dyh2_A3 = 1\n",
    "        dyh1_A2 = dyh2_A4 = -1\n",
    "        dyh1_A3 = dyh2_A1 = 0\n",
    "        dyh1_A4 = dyh2_A2 = 0\n",
    "\n",
    "        \n",
    "        dA1_w1 = torch.from_numpy(dRelu(self.ch['Z1'][0]))\n",
    "        dA2_w3 = torch.from_numpy((dRelu(self.ch['Z1'][0])-1))\n",
    "        \n",
    "        dA1_w2 = torch.from_numpy(dRelu(self.ch['Z1'][0]))\n",
    "        dA2_w4 = torch.from_numpy((dRelu(self.ch['Z1'][0])-1))\n",
    "        \n",
    "        dA3_w5 = torch.from_numpy(dRelu(self.ch['Z1'][2]))\n",
    "        dA4_w7 = torch.from_numpy(dRelu(self.ch['Z1'][2])-1)\n",
    "        \n",
    "        dA3_w6 = torch.from_numpy(dRelu(self.ch['Z1'][2]))\n",
    "        dA4_w8 = torch.from_numpy(dRelu(self.ch['Z1'][2])-1)\n",
    "        \n",
    "        dloss_w1 = np.matmul(dloss_yh1*(dyh1_A1*dA1_w1+dyh1_A2*dA2_w3),X1.T)\n",
    "        dloss_w2 = np.matmul(dloss_yh1*(dyh1_A1*dA1_w2+dyh1_A2*dA2_w4),X2.T)\n",
    "        dloss_w5 = np.matmul(dloss_yh2*(dyh2_A3*dA3_w5+dyh2_A4*dA4_w7),X1.T)\n",
    "        dloss_w6 = np.matmul(dloss_yh2*(dyh2_A3*dA3_w6+dyh2_A4*dA4_w8),X2.T)\n",
    "\n",
    "        dloss_W1 = torch.from_numpy(np.array([[dloss_w1,dloss_w2],[-dloss_w1,-dloss_w2],[dloss_w5,dloss_w6],[-dloss_w5,-dloss_w6]]))  #(4,2)\n",
    "\n",
    "        self.weights[\"W1\"] = self.weights[\"W1\"] - self.lr * dloss_W1\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        prediction = np.matmul(self.weights['W2'],Relu(np.matmul(self.weights['W1'],test_data)))\n",
    "        return self.calculate_loss(prediction)\n",
    "    \n",
    "    def run(self,X, Y, epochs):\n",
    "        self.loss = []\n",
    "        self.weightlist = []\n",
    "        np.random.seed(1)                         \n",
    "        self.nInit()\n",
    "        for i in range(0, epochs):\n",
    "            Yh, loss=self.forward()\n",
    "            self.backward()\n",
    "            self.loss.append(loss)\n",
    "            if i >= 300:\n",
    "                self.weightlist.append(self.weights[\"W1\"].numpy())\n",
    "                \n",
    "#         print(\"-\"*50)\n",
    "#         print(\"W1\",self.weights[\"W1\"],\"\\n\")\n",
    "#         print(\"Y\",Y)\n",
    "#         print(\"Yh\",Yh)\n",
    "\n",
    "        return self.loss,self.weightlist\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n,epochs):\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    train_x_all, train_y_1,train_y_01,train_y_001,train_y_0001= data_generator(n) # create the data \n",
    "    test_x_all, test_y_1,test_y_01,test_y_001,test_y_0001=data_generator(n)\n",
    "    \n",
    "    \n",
    "    nn = net(train_x_all,train_y_1)\n",
    "    list_1,w_list_1 = nn.run(train_x_all,train_y_1,epochs)\n",
    "    print(\"test_1_NMSE-----\",nn.predict(test_y_1))\n",
    "#     print(list_1)\n",
    "\n",
    "    nn = net(train_x_all,train_y_01)\n",
    "    list_01,w_list_01 = nn.run(train_x_all,train_y_01,epochs)\n",
    "    print(\"test_01_NMSE-----\",nn.predict(test_y_01))\n",
    "#     print(list_01)\n",
    "\n",
    "    nn = net(train_x_all,train_y_001)\n",
    "    list_001,w_list_001 = nn.run(train_x_all,train_y_001,epochs)\n",
    "    print(\"test_001_NMSE-----\",nn.predict(test_y_001))\n",
    "#     print(list_001)\n",
    "\n",
    "    nn = net(train_x_all,train_y_0001)\n",
    "    list_0001,w_list_0001 = nn.run(train_x_all,train_y_0001,epochs)\n",
    "    print(\"test_0001_NMSE-----\",nn.predict(test_y_0001))\n",
    "    \n",
    "    for w in [w_list_1,w_list_01,w_list_001,w_list_0001]:\n",
    "        mean = np.sum(w,axis=0)/len(w)\n",
    "        mean0fsquared_w = np.sum(np.square(w),axis=0)/len(w)\n",
    "        \n",
    "        print(\"~\"*50)\n",
    "        print(\"MEAN------\",mean)\n",
    "        print(\"VARIENCE------\",mean0fsquared_w-np.square(mean))\n",
    "    \n",
    "    # plot 3000 epochs and the losses\n",
    "    epochs = 3000\n",
    "    plt.suptitle('Forward Problem')\n",
    "    plt.plot(range(epochs),list_1)\n",
    "    plt.plot(range(epochs),list_01)\n",
    "    plt.plot(range(epochs),list_001)\n",
    "    plt.plot(range(epochs),list_0001)\n",
    "#     x1,x2,y1,y2 = plt.axis()\n",
    "#     plt.axis((x1,x2,0,14))\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Normalised Mean Squared Error')\n",
    "    plt.legend(['1', '0.1', '0.01', '0.0001'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "#     plot the first 1000 epochs\n",
    "    epochs = 1000\n",
    "    plt.suptitle('Forward Problem')\n",
    "    plt.plot(range(epochs),list_1[:1000])\n",
    "    plt.plot(range(epochs),list_01[:1000])\n",
    "    plt.plot(range(epochs),list_001[:1000])\n",
    "    plt.plot(range(epochs),list_0001[:1000])\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((x1,x2,0,0.1))\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Normalised Mean Squared Error')\n",
    "    plt.legend(['1', '0.1', '0.01', '0.0001'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_main(n,epochs):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    train_x_all, train_y_1,train_y_01,train_y_001,train_y_0001= data_generator(n) # create the data \n",
    "    test_x_all, test_y_1,test_y_01,test_y_001,test_y_0001=data_generator(n)\n",
    "    \n",
    "    nn = net(train_y_1, train_x_all)\n",
    "    list_1,w_list_1 = nn.run(train_y_1, train_x_all,epochs)\n",
    "    #print(\"test_1_NMSE-----\",nn.predict(test_x_all))\n",
    "    \n",
    "    x_t1 = train_x_all[0]\n",
    "    x_t2 = train_x_all[1]\n",
    "    x1= nn.Yh.numpy()[0]\n",
    "    x2= nn.Yh.numpy()[1]\n",
    "\n",
    "    plt.scatter(x_t1,x_t2,color=\"blue\",alpha=0.2)\n",
    "    plt.suptitle('The distribution of the estimated x (epsilon=1) - random data')\n",
    "    plt.scatter(x1,x2,color=\"red\",alpha=0.2)\n",
    "    plt.legend(['original x', 'estimated x'], loc='upper right')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "\n",
    "    nn = net(train_y_01, train_x_all)\n",
    "    list_01,w_list_01 = nn.run(train_y_01, train_x_all,epochs)\n",
    "    #print(\"test_01_NMSE-----\",nn.predict(test_x_all))\n",
    "    \n",
    "    x1= nn.Yh.numpy()[0]\n",
    "    x2= nn.Yh.numpy()[1]\n",
    "    plt.suptitle('The distribution of the estimated x (epsilon=0.1) - random data')\n",
    "    plt.scatter(x_t1,x_t2,color=\"blue\",alpha=0.2)\n",
    "    plt.scatter(x1,x2,color=\"red\",alpha=0.2)\n",
    "    plt.legend(['original x', 'estimated x'], loc='upper right')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "    \n",
    "    nn = net(train_y_001, train_x_all)\n",
    "    list_001,w_list_001 = nn.run(train_y_001, train_x_all,epochs)\n",
    "    #print(\"test_001_NMSE-----\",nn.predict(test_x_all))\n",
    "    \n",
    "    x1= nn.Yh.numpy()[0]\n",
    "    x2= nn.Yh.numpy()[1]\n",
    "    plt.suptitle('The distribution of the estimated x (epsilon=0.01) - random data')\n",
    "    plt.scatter(x_t1,x_t2,color=\"blue\",alpha=0.2)\n",
    "    plt.scatter(x1,x2,color=\"red\",alpha=0.2)\n",
    "    plt.legend(['original x', 'estimated x'], loc='upper right')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "    \n",
    "    nn = net(train_y_0001, train_x_all)\n",
    "    list_0001,w_list_0001 = nn.run(train_y_0001, train_x_all,epochs)\n",
    "    #print(\"test_0001_NMSE-----\",nn.predict(test_x_all))\n",
    "    \n",
    "    x1= nn.Yh.numpy()[0]\n",
    "    x2= nn.Yh.numpy()[1]\n",
    "    plt.suptitle('The distribution of the estimated x (epsilon=0.0001) - random data')\n",
    "    plt.scatter(x_t1,x_t2,color=\"blue\",alpha=0.2)\n",
    "    plt.scatter(x1,x2,color=\"red\",alpha=0.2)\n",
    "    plt.legend(['original x', 'estimated x'], loc='upper right')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"The mean and varience of the pseudo-inverse -stmmetric network with random data\")\n",
    "    for w in [w_list_1,w_list_01,w_list_001,w_list_0001]:\n",
    "        mean = np.sum(w,axis=0)/len(w)\n",
    "        mean0fsquared_w = np.sum(np.square(w),axis=0)/len(w)\n",
    "        \n",
    "        print(\"~\"*50)\n",
    "        print(\"MEAN------\",mean)\n",
    "        print(\"VARIENCE------\",mean0fsquared_w-np.square(mean))\n",
    "        \n",
    "    # plot 3000 epochs and the losses\n",
    "    epochs = 3000\n",
    "    plt.suptitle('Inverse Problem - python')\n",
    "    plt.plot(range(epochs),list_1)\n",
    "    plt.plot(range(epochs),list_01)\n",
    "    plt.plot(range(epochs),list_001)\n",
    "    plt.plot(range(epochs),list_0001)\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((x1,x2,0,14))\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Normalised Mean Squared Error')\n",
    "    plt.legend(['1', '0.1', '0.01', '0.0001'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main(10000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_main(10000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tick_inverse_main(n,epochs):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    train_x_all, train_y_1,train_y_01,train_y_001,train_y_0001= data_generator(n) # create the data \n",
    "    \n",
    "    for index,Y in enumerate([train_y_1,train_y_01,train_y_001,train_y_0001]):\n",
    "        globals()['x_tik_'+str(index)] = np.matmul(np.matmul(inv(np.matmul(As[index].T,As[index])\n",
    "                                                                 +0.04**2*np.eye(2)),As[index].T),Y)\n",
    "        \n",
    "    \n",
    "    nn = net(train_y_1,x_tik_0)\n",
    "    list_1,w_list_1 = nn.run(train_y_1,x_tik_0,epochs)\n",
    "    \n",
    "    x_t1 = x_tik_0[0]\n",
    "    x_t2 = x_tik_0[1]\n",
    "    x1= nn.Yh.numpy()[0]\n",
    "    x2= nn.Yh.numpy()[1]\n",
    "    plt.suptitle('The distribution of the estimated x_tik (epsilon=1) - regularised data')\n",
    "    plt.scatter(x_t1,x_t2,color=\"blue\",alpha=0.2)\n",
    "    plt.scatter(x1,x2,color=\"red\",alpha=0.2)\n",
    "    plt.legend(['original x_tik', 'estimated x_tik'], loc='upper right')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "    \n",
    "    nn = net(train_y_01, x_tik_1)\n",
    "    list_01,w_list_01 = nn.run(train_y_01, x_tik_1,epochs)\n",
    "    \n",
    "    x_t1 = x_tik_1[0]\n",
    "    x_t2 = x_tik_1[1]\n",
    "    x1= nn.Yh.numpy()[0]\n",
    "    x2= nn.Yh.numpy()[1]\n",
    "    plt.suptitle('The distribution of the estimated x_tik (epsilon=0.1) - regularised data')\n",
    "    plt.scatter(x_t1,x_t2,color=\"blue\",alpha=0.2)\n",
    "    plt.scatter(x1,x2,color=\"red\",alpha=0.2)\n",
    "    plt.legend(['original x_tik', 'estimated x_tik'], loc='upper right')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "    \n",
    "    nn = net(train_y_001, x_tik_2)\n",
    "    list_001,w_list_001 = nn.run(train_y_001, x_tik_2,epochs)\n",
    "    #print(list_001)\n",
    "    \n",
    "    x_t1 = x_tik_2[0]\n",
    "    x_t2 = x_tik_2[1]\n",
    "    x1= nn.Yh.numpy()[0]\n",
    "    x2= nn.Yh.numpy()[1]\n",
    "    plt.suptitle('The distribution of the estimated x_tik (epsilon=0.01) - regularised data')\n",
    "    plt.scatter(x_t1,x_t2,color=\"blue\",alpha=0.2)\n",
    "    plt.scatter(x1,x2,color=\"red\",alpha=0.2)\n",
    "    plt.legend(['original x_tik', 'estimated x_tik'], loc='upper right')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "    \n",
    "    nn = net(train_y_0001, x_tik_3)\n",
    "    list_0001,w_list_0001 = nn.run(train_y_0001, x_tik_3,epochs)  \n",
    "    \n",
    "    x_t1 = x_tik_3[0]\n",
    "    x_t2 = x_tik_3[1]\n",
    "    x1= nn.Yh.numpy()[0]\n",
    "    x2= nn.Yh.numpy()[1]\n",
    "    plt.suptitle('The distribution of the estimated x_tik (epsilon=0.0001) - regularised data')\n",
    "    plt.scatter(x_t1,x_t2,color=\"blue\",alpha=0.2)\n",
    "    plt.scatter(x1,x2,color=\"red\",alpha=0.2)\n",
    "    plt.legend(['original x_tik', 'estimated x_tik'], loc='upper right')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"The mean and varience of the pseudo-inverse -stmmetric network with regularised data\")\n",
    "    for w in [w_list_1,w_list_01,w_list_001,w_list_0001]:\n",
    "        mean = np.sum(w,axis=0)/len(w)\n",
    "        mean0fsquared_w = np.sum(np.square(w),axis=0)/len(w)\n",
    "        \n",
    "        print(\"~\"*50)\n",
    "        print(\"MEAN------\",mean)\n",
    "        print(\"VARIENCE------\",mean0fsquared_w-np.square(mean))\n",
    "\n",
    "    # plot 3000 epochs and the losses\n",
    "    epochs = 3000\n",
    "    plt.suptitle('Inverse Problem(tikhonov)- python')\n",
    "    plt.plot(range(epochs),list_1)\n",
    "    plt.plot(range(epochs),list_01)\n",
    "    plt.plot(range(epochs),list_001)\n",
    "    plt.plot(range(epochs),list_0001)\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "#     plt.axis((x1,x2,0,14))\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Normalised Mean Squared Error')\n",
    "    plt.legend(['original x_tik', 'estimated x_tik'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,A in enumerate(As):\n",
    "        globals()['tick_approx_'+str(index)] = np.matmul(inv(np.matmul(As[index].T,As[index])+0.04**2*np.eye(2)),As[index].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tick_approx_1\",tick_approx_0,\"\\n\")\n",
    "print(\"tick_approx_0.1\",tick_approx_1,\"\\n\")\n",
    "print(\"tick_approx_0.01\",tick_approx_2,\"\\n\")\n",
    "print(\"tick_approx_0.0001\",tick_approx_3,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_inverse_main(10000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
